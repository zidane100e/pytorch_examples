{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Pytorch\n",
    "## example : Yelp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data as ttdata\n",
    "from torchtext.data import Dataset as ttDataset\n",
    "from torchtext.data import Dataset, Example, Field\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import spacy\n",
    "\n",
    "from TextDataloader import TextData\n",
    "\n",
    "from generate_model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "elif torch.cuda.device_count()>0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataload\n",
    "* reference : Natural language processing with PYTORCH\n",
    "1. load data file into a dataframe\n",
    "2. change text into right form using tokenizer or corpus\n",
    "3. change modified text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data shape\n",
    "rating, review, split\n",
    "negative, \"sentence\", train\n",
    "positive, \"sentence\", train\n",
    "\"\"\"\n",
    "f1_s = \"/home/bwlee/data/yelp_review_polarity_csv/reviews_with_splits_full.csv\"\n",
    "df = pd.read_csv(f1_s, header=0, skiprows=lambda x: x%10>0)\n",
    "for i in df.index: \n",
    "    if df['rating'].iloc[i] == 'positive':\n",
    "        df['rating'].iloc[i] = 1\n",
    "    else:\n",
    "        df['rating'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split']=='train'][['review', 'rating']]\n",
    "val_df = df[df['split']=='val'][['review', 'rating']]\n",
    "test_df = df[df['split']=='test'][['review', 'rating']]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went here with another couple for restaurant week . the place was fairly busy . we waited for mins and were seated . we had a reservation , but that didn t seem to affect the folks seating tables . we got to our table outside wand waited another half hour just to see our waitress . we noticed everyone around us was complaining about the slow service . we ordered drinks and about another half hour later we ordered our food . my lobster was overcooked and stringy and my steak was mediocre at best . all in all my visit to the mariner s inn was terrible . \n"
     ]
    }
   ],
   "source": [
    "print(train_df.review[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ttdata.Field(sequential=True, use_vocab=True,\n",
    "                 tokenize='spacy', lower=True,\n",
    "                 batch_first=True, fix_length=100,\n",
    "                 init_token='<SOS>', eos_token='<EOS>')\n",
    "\n",
    "LABEL = ttdata.Field(sequential=False, use_vocab=False,\n",
    "                  batch_first=True, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields0 = (('review', TEXT), ('rating', LABEL))\n",
    "train_data0 = [ Example.fromlist(\n",
    "    train_df.iloc[ii].values.tolist(),\n",
    "                              fields0)\n",
    "              for ii in range(len(train_df)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torchtext.data.example.Example at 0x7f85829516a0>]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(train_data)\n",
    "train_data = ttDataset(train_data0[:n_data//5*4], fields=fields0)\n",
    "test_data = ttDataset(train_data0[n_data//5*4:], fields=fields0)\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = Iterator(train_data, batch_size=100, shuffle=True)\n",
    "iter_test = Iterator(test_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, model=None, loss=None, \n",
    "                 optimizer=None, device='cuda'):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "    \n",
    "    def run_batch(self, i_batch, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in, tgt = data\n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        out = self.model(data_in)\n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def run_train(self, n_epoch, data, test_data=None):\n",
    "        self.model.train()\n",
    "        for i_epoch in range(n_epoch):\n",
    "            loss = 0\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                loss_temp = self.run_batch(i_batch, data_batch)\n",
    "                loss += loss_temp\n",
    "            loss /= 1.0*len(data)\n",
    "            print('epoch', i_epoch, 'loss', loss)\n",
    "            \n",
    "        if test_data is None:\n",
    "            return self.run_eval(test_data)\n",
    "        else:\n",
    "            return self.run_eval(data)\n",
    "        \n",
    "    def run_eval(self, data):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in, tgt = data_batch\n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                out = self.model(data_in)\n",
    "                loss += self.loss(out, tgt).detach().cpu()\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                else:\n",
    "                    outs = torch.cat((outs, out), dim=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss)\n",
    "        return outs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(Net):\n",
    "    def __init__(self, embed = None, rnn=None, downnet=None,\n",
    "                 loss=None, optimizer=None, device='cuda'):\n",
    "        super(RNN_classifier, self).__init__()\n",
    "        self.embed = embed\n",
    "        self.rnn = rnn\n",
    "        self.downnet = downnet\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model = self.rnn\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        torch.nn.init.xavier_uniform_(self.downnet[1].weight)\n",
    "        #self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        #self.fc.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.embed is not None:\n",
    "            x = self.embed(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.downnet(out[:,-1]) # choose last output\n",
    "    \n",
    "    def run_batch(self, i_batch, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in, tgt = data\n",
    "        data_in = data_in.to(device)\n",
    "        data_temp = data_in\n",
    "        tgt = tgt.to(device)        \n",
    "        if self.embed is not None:\n",
    "            data_in = self.embed(data_in)\n",
    "        out, hidden = self.rnn(data_in)\n",
    "        temp = out[:,-1]\n",
    "        out = self.downnet(out[:,-1]) # choose last output\n",
    "        #\"\"\"\n",
    "        if i_batch % 10 == 0:\n",
    "            print('in', data_temp[:2,:4])\n",
    "            print(temp[:2,:3])\n",
    "            print(data_in[:2,:4,:3])\n",
    "            print('out', out[:5], tgt[:5])\n",
    "            print()\n",
    "        #\"\"\"\n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def run_train(self, n_epoch, data, test_data=None):\n",
    "        self.embed.train()\n",
    "        self.rnn.train()\n",
    "        self.downnet.train()\n",
    "        return super().run_train(n_epoch, data, test_data)\n",
    "    \n",
    "    def run_eval(self, data):\n",
    "        self.embed.eval()\n",
    "        self.rnn.eval()\n",
    "        self.downnet.eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in, tgt = data_batch\n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                \n",
    "                if self.embed is not None:\n",
    "                    data_in = self.embed(data_in)\n",
    "                out, hidden = self.rnn(data_in)\n",
    "                out = self.downnet(out[:,-1]) # choose last output\n",
    "                \n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = torch.cat((outs, out), dim=0)\n",
    "                    tgts = torch.cat((tgts, tgt), dim=0)\n",
    "        loss /= 1.0*i_batch\n",
    "        print('evaluate---', 'loss', loss)\n",
    "        outs_np = outs.cpu().numpy()\n",
    "        tgts_np = tgts.cpu().numpy()\n",
    "        \n",
    "        print('accuracy', accuracy(outs_np, tgts_np))\n",
    "        return outs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embed = 200\n",
    "dim_hidden = 200\n",
    "embed = nn.Embedding(num_embeddings=len(TEXT.vocab),\n",
    "                    embedding_dim=dim_embed,\n",
    "                    padding_idx=TEXT.vocab.stoi['<pad>']).to(device)\n",
    "\n",
    "lstm = nn.LSTM(input_size=dim_embed, \n",
    "        hidden_size=dim_hidden, \n",
    "        num_layers= 2,\n",
    "        batch_first=True,\n",
    "        bidirectional=True).to(device)\n",
    "\n",
    "#classifier = get_MLP([dim_hidden*2, dim_hidden*3, dim_hidden, 2])\n",
    "classifier = get_MLP([dim_hidden*2, 2], end=True)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "#net = nn.Sequential(embed, lstm, classifier)\n",
    "#get_RNN(10, 20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "parms = list(embed.parameters())\n",
    "parms += list(lstm.parameters())\n",
    "parms += list(classifier.parameters())\n",
    "\n",
    "optimizer = optim.Adam(parms)\n",
    "seq_class = RNN_classifier(embed, lstm, classifier, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_train() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-272-19bfad3c5aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run_train() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "seq_class.run_train(1, iter_train, iter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.6377113946542448\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.7318367346938776\n",
      "epoch 0 loss 0.43713363054759646\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.8792091836734693\n",
      "epoch 0 loss 0.27339637579814513\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.898469387755102\n",
      "epoch 0 loss 0.21460443157322553\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9386734693877551\n",
      "epoch 0 loss 0.17209600320808133\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9575255102040816\n",
      "epoch 0 loss 0.1389554777486744\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9680102040816326\n",
      "epoch 0 loss 0.10917216761284793\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9788775510204082\n",
      "epoch 0 loss 0.08162904679727721\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9871683673469388\n",
      "epoch 0 loss 0.05522642822755615\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9913265306122448\n",
      "epoch 0 loss 0.05046790648056954\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9929591836734694\n",
      "epoch 0 loss 0.032294202867980897\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9910459183673469\n",
      "epoch 0 loss 0.025824555308868093\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.993545918367347\n",
      "epoch 0 loss 0.022624165726242332\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.994030612244898\n",
      "epoch 0 loss 0.023848164986851102\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9753571428571428\n",
      "epoch 0 loss 0.030710614646485904\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9966581632653061\n",
      "epoch 0 loss 0.012928412157337761\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9954336734693877\n",
      "epoch 0 loss 0.009807714645589561\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9972704081632653\n",
      "epoch 0 loss 0.010439585549035228\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9980357142857142\n",
      "epoch 0 loss 0.009035491559560035\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9984438775510204\n",
      "epoch 0 loss 0.010132657547821547\n",
      "evaluate--- loss 0.0\n",
      "accuracy 0.9982142857142857\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    seq_class.run_train(1, iter1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
