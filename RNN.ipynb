{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN in Pytorch\n",
    "## example : Yelp classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data as ttdata\n",
    "from torchtext.data import Dataset as ttDataset\n",
    "from torchtext.data import Dataset, Example, Field\n",
    "from torchtext.data import Iterator, BucketIterator\n",
    "import spacy\n",
    "\n",
    "from TextDataloader import TextData\n",
    "\n",
    "from generate_model import *\n",
    "from kbutils.evaluation import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "elif torch.cuda.device_count()>0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataload\n",
    "* reference : Natural language processing with PYTORCH\n",
    "1. load data file into a dataframe\n",
    "2. change text into right form using tokenizer or corpus\n",
    "3. change modified text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data shape\n",
    "rating, review, split\n",
    "negative, \"sentence\", train\n",
    "positive, \"sentence\", train\n",
    "\"\"\"\n",
    "f1_s = \"/home/bwlee/data/yelp_review_polarity_csv/reviews_with_splits_full.csv\"\n",
    "df = pd.read_csv(f1_s, header=0, skiprows=lambda x: x%10>0)\n",
    "for i in df.index: \n",
    "    if df['rating'].iloc[i] == 'positive':\n",
    "        df['rating'].iloc[i] = 1\n",
    "    else:\n",
    "        df['rating'].iloc[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split']=='train'][['review', 'rating']]\n",
    "val_df = df[df['split']=='val'][['review', 'rating']]\n",
    "test_df = df[df['split']=='test'][['review', 'rating']]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went here with another couple for restaurant week . the place was fairly busy . we waited for mins and were seated . we had a reservation , but that didn t seem to affect the folks seating tables . we got to our table outside wand waited another half hour just to see our waitress . we noticed everyone around us was complaining about the slow service . we ordered drinks and about another half hour later we ordered our food . my lobster was overcooked and stringy and my steak was mediocre at best . all in all my visit to the mariner s inn was terrible . \n"
     ]
    }
   ],
   "source": [
    "print(train_df.review[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = ttdata.Field(sequential=True, use_vocab=True,\n",
    "                 tokenize='spacy', lower=True,\n",
    "                 batch_first=True, fix_length=100,\n",
    "                 init_token='<SOS>', eos_token='<EOS>')\n",
    "\n",
    "LABEL = ttdata.Field(sequential=False, use_vocab=False,\n",
    "                  batch_first=True, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields0 = (('review', TEXT), ('rating', LABEL))\n",
    "iis = list(range(len(train_df)))\n",
    "random.shuffle(iis)\n",
    "train_data0 = [ Example.fromlist(\n",
    "    train_df.iloc[ii].values.tolist(),\n",
    "                              fields0) for ii in iis ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = len(train_data)\n",
    "train_data = ttDataset(train_data0[:n_data//5*4], fields=fields0)\n",
    "test_data = ttDataset(train_data0[n_data//5*4:], fields=fields0)\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = Iterator(train_data, batch_size=100, shuffle=True)\n",
    "iter_test = Iterator(test_data, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(Net):\n",
    "    def __init__(self, embed = None, rnn=None, downnet=None,\n",
    "                 loss=None, optimizer=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        net is consists of [embed, rnn, downnet]\n",
    "        :param downnet: define downstream job\n",
    "        \"\"\"   \n",
    "        super(RNN_classifier, self).__init__(loss=loss,\n",
    "                                            optimizer=optimizer,\n",
    "                                            device=device)\n",
    "        self.embed = embed\n",
    "        self.rnn = rnn\n",
    "        self.downnet = downnet\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def set_train(self):\n",
    "        self.embed.train()\n",
    "        self.rnn.train()\n",
    "        self.downnet.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.embed.eval()\n",
    "        self.rnn.eval()\n",
    "        self.downnet.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        torch.nn.init.xavier_uniform_(self.downnet[1].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.embed is not None:\n",
    "            x = self.embed(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = self.downnet(out[:,-1]) # choose last output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_embed = 100\n",
    "dim_hidden = 200\n",
    "embed = nn.Embedding(num_embeddings=len(TEXT.vocab),\n",
    "                    embedding_dim=dim_embed,\n",
    "                    padding_idx=TEXT.vocab.stoi['<pad>']).to(device)\n",
    "\n",
    "lstm = nn.LSTM(input_size=dim_embed, \n",
    "        hidden_size=dim_hidden, \n",
    "        num_layers= 2,\n",
    "        batch_first=True,\n",
    "        bidirectional=True).to(device)\n",
    "\n",
    "classifier = get_MLP([dim_hidden*2, dim_hidden*3, dim_hidden, 2], end=True)\n",
    "#classifier = get_MLP([dim_hidden*2, 2], end=True)\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "parms = list(embed.parameters())\n",
    "parms += list(lstm.parameters())\n",
    "parms += list(classifier.parameters())\n",
    "\n",
    "optimizer = optim.Adam(parms)\n",
    "seq_class = RNN_classifier(embed, lstm, classifier, loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.6797516005379813\n",
      "evaluate mom loss tensor(0.6868)\n",
      "accuracy 0.5300241921548298\n",
      "epoch 0 loss 0.5731219926976269\n",
      "evaluate mom loss tensor(0.4280)\n",
      "accuracy 0.8170468290997063\n",
      "epoch 0 loss 0.33886991765188135\n",
      "evaluate mom loss tensor(0.3308)\n",
      "accuracy 0.8513046483497494\n",
      "epoch 0 loss 0.229146563118289\n",
      "evaluate mom loss tensor(0.3187)\n",
      "accuracy 0.8649991359944703\n",
      "epoch 0 loss 0.17484407871961594\n",
      "evaluate mom loss tensor(0.3177)\n",
      "accuracy 0.8749783998617591\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    seq_class.run_train(1, iter_train, iter_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
