{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def int2onehot(x, n_class):\n",
    "    ret = torch.zeros(n_class)\n",
    "    ret[x] = 1\n",
    "    return ret\n",
    "int2onehot(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def int2onehot(x, n_class):\n",
    "    ret = torch.zeros(n_class)\n",
    "    ret[x] = 1\n",
    "    return ret\n",
    "    \n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "# if not exist, download mnist dataset\n",
    "dir_s = '/home/bwlee/data'\n",
    "train_set = dset.MNIST(root=dir_s, train=True, transform=trans, download=True)\n",
    "test_set = dset.MNIST(root=dir_s, train=False, transform=trans, download=True)\n",
    "# change 2D to 1D for MLP\n",
    "# train_set\n",
    "# [(tensor(batch, channel, x, y), label_one_hot)]\n",
    "# train_set2\n",
    "# [(tensor(batch, channel, x*y), label_one_hot)] \n",
    "n_class = 10\n",
    "train_set2, test_set2 = [], []\n",
    "for data in train_set:\n",
    "    #train_set2.append([data[0].view(1,-1), int2onehot(data[1], n_class)])\n",
    "    train_set2.append([data[0].view(-1), data[1]])\n",
    "for data in test_set:\n",
    "    #test_set2.append([data[0].view(1,-1), int2onehot(data[1], n_class)])\n",
    "    test_set2.append([data[0].view(-1), data[1]])\n",
    "    \n",
    "batch_size = 512\n",
    "# train_loader, test_loader has form of [data_batch, tgt_batch]\n",
    "# data_batch in [batch, channel, x, y]\n",
    "# data_batch in [batch]\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set2,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "2\n",
      "10000\n",
      "torch.Size([784])\n",
      "2\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(test_set[1][0].size())\n",
    "print(test_set[1][1])\n",
    "print(len(test_set))\n",
    "print(test_set2[1][0].size())\n",
    "print(test_set2[1][1])\n",
    "print(len(test_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 784])\n",
      "torch.Size([512])\n",
      "tensor([[-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
      "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000],\n",
      "        [-0.5000, -0.5000, -0.5000,  ..., -0.5000, -0.5000, -0.5000]])\n",
      "tensor([7, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    if i>0: break\n",
    "    print(data[0].size())\n",
    "    print(data[1].size())\n",
    "    print(data[0][:3])\n",
    "    print(data[1][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MLP(n_hiddens, activation=nn.ReLU(), dropout=0.1):\n",
    "    def get_a_layer(n_in, n_out, activation, dropout):\n",
    "        seq = [nn.Dropout(dropout), nn.Linear(n_in, n_out),\n",
    "                activation]\n",
    "        return seq\n",
    "    layers = [get_a_layer(n_in, n_out, activation, dropout) for \n",
    "              n_in, n_out in zip(n_hiddens, n_hiddens[1:])]\n",
    "    layers = [ x for xs in layers for x in xs ]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model=None, loss=None, \n",
    "                 optimizer=None):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def run_batch(self, i_batch, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in, tgt = data\n",
    "        data_in = data_in.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        out = self.model(data_in)\n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def run_train(self, n_epoch, data):\n",
    "        self.model.train()\n",
    "        for i_epoch in range(n_epoch):\n",
    "            loss = 0\n",
    "            n_batch = len(data)\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                loss += self.run_batch(i_batch, data_batch)\n",
    "            #print(i_batch, n_batch)\n",
    "            loss /= 1.0*n_batch\n",
    "            print('epoch', i_epoch, 'loss', loss)\n",
    "            \n",
    "    def run_eval(self, data):\n",
    "        self.model.eval()\n",
    "        loss = 0\n",
    "        for i_batch, data_batch in enumerate(data):\n",
    "            data_in, tgt = data_batch\n",
    "            out = self.model(data_in)\n",
    "            loss += self.loss(out, tgt).detach().cpu()\n",
    "        loss /= 1.0*i_batch\n",
    "        return out, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, model=None, loss=None, \n",
    "                 optimizer=None):\n",
    "        super(Autoencoder, self).__init__(model, loss, optimizer)\n",
    "    \n",
    "    def run_batch(self, i_batch, data):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in, _ = data\n",
    "        data_in = data_in.to(device)\n",
    "        out = self.model(data_in)\n",
    "        loss = self.loss(out, data_in)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = get_MLP([100, 200, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=200, out_features=50, bias=True)\n",
       "  (5): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_mnist = 784\n",
    "\n",
    "encoder = get_MLP([784, 300, 100, 10])\n",
    "#decoder = get_MLP([100, 300, 784])\n",
    "#ae_model = nn.Sequential(encoder, decoder)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(encoder.parameters())\n",
    "classifier = Model(model=encoder, \n",
    "                loss=loss, optimizer=optimizer)\n",
    "classifier.run_train(20, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.23071343237060613\n",
      "epoch 1 loss 0.2276317797980066\n",
      "epoch 2 loss 0.22480619521969455\n",
      "epoch 3 loss 0.2231647442963164\n",
      "epoch 4 loss 0.22215470797934775\n",
      "epoch 5 loss 0.22136260569095612\n",
      "epoch 6 loss 0.22082046267844863\n",
      "epoch 7 loss 0.22053612200385433\n",
      "epoch 8 loss 0.2203439912300999\n",
      "epoch 9 loss 0.22011182154134168\n",
      "epoch 10 loss 0.21999640005119778\n",
      "epoch 11 loss 0.2198753744616347\n",
      "epoch 12 loss 0.21977928886979314\n",
      "epoch 13 loss 0.2196949321334645\n",
      "epoch 14 loss 0.21961670217372603\n",
      "epoch 15 loss 0.21953157941668722\n",
      "epoch 16 loss 0.21940884991722592\n",
      "epoch 17 loss 0.2193568220077935\n",
      "epoch 18 loss 0.21931206763295805\n",
      "epoch 19 loss 0.2192240796856961\n",
      "epoch 20 loss 0.21917644585088147\n",
      "epoch 21 loss 0.21914299918433366\n",
      "epoch 22 loss 0.21911113077806213\n",
      "epoch 23 loss 0.21905980132899042\n",
      "epoch 24 loss 0.21902856546438346\n",
      "epoch 25 loss 0.21899219387668675\n",
      "epoch 26 loss 0.21892157247511007\n",
      "epoch 27 loss 0.21891247165405145\n",
      "epoch 28 loss 0.21887813685304028\n",
      "epoch 29 loss 0.21888107617022629\n",
      "epoch 30 loss 0.21884600641363758\n",
      "epoch 31 loss 0.2188180639582165\n",
      "epoch 32 loss 0.21879464463662293\n",
      "epoch 33 loss 0.2187732490947691\n",
      "epoch 34 loss 0.21874926302392603\n",
      "epoch 35 loss 0.21871507496146833\n",
      "epoch 36 loss 0.21864692399562416\n",
      "epoch 37 loss 0.2186241804038064\n",
      "epoch 38 loss 0.2186198797771486\n",
      "epoch 39 loss 0.21860510161367513\n",
      "epoch 40 loss 0.21859810854923928\n",
      "epoch 41 loss 0.21858417419558865\n",
      "epoch 42 loss 0.21856209762015585\n",
      "epoch 43 loss 0.21856328074709844\n",
      "epoch 44 loss 0.21854725492707752\n",
      "epoch 45 loss 0.2184547125282934\n",
      "epoch 46 loss 0.21836611413854665\n",
      "epoch 47 loss 0.21833452392937774\n",
      "epoch 48 loss 0.21831631205849727\n",
      "epoch 49 loss 0.21831971268027517\n",
      "epoch 50 loss 0.21830546035099838\n",
      "epoch 51 loss 0.21830354504666086\n",
      "epoch 52 loss 0.21828260777865427\n",
      "epoch 53 loss 0.21826986387624578\n",
      "epoch 54 loss 0.2182078237755824\n",
      "epoch 55 loss 0.21817818132497496\n",
      "epoch 56 loss 0.21817174667524078\n",
      "epoch 57 loss 0.21814254747103837\n",
      "epoch 58 loss 0.21811816917132523\n",
      "epoch 59 loss 0.21811392676022093\n",
      "epoch 60 loss 0.21804514678858095\n",
      "epoch 61 loss 0.21797898121304432\n",
      "epoch 62 loss 0.21794844545045142\n",
      "epoch 63 loss 0.21795515097298865\n",
      "epoch 64 loss 0.21793337848226904\n",
      "epoch 65 loss 0.21792714954432796\n",
      "epoch 66 loss 0.21792576613567644\n",
      "epoch 67 loss 0.217919516992771\n",
      "epoch 68 loss 0.2178706150438826\n",
      "epoch 69 loss 0.2178660571827727\n",
      "epoch 70 loss 0.217859960713629\n",
      "epoch 71 loss 0.21785674951339173\n",
      "epoch 72 loss 0.21784194091619072\n",
      "epoch 73 loss 0.2178361131730726\n",
      "epoch 74 loss 0.21781657573025107\n",
      "epoch 75 loss 0.21782971552367938\n",
      "epoch 76 loss 0.21781458970853837\n",
      "epoch 77 loss 0.21781328358387542\n",
      "epoch 78 loss 0.21780872307086396\n",
      "epoch 79 loss 0.21778583084627734\n",
      "epoch 80 loss 0.2177780852732012\n",
      "epoch 81 loss 0.2177388132628748\n",
      "epoch 82 loss 0.2177187153848551\n",
      "epoch 83 loss 0.2177213252348415\n",
      "epoch 84 loss 0.21771612094115403\n",
      "epoch 85 loss 0.21770889375169397\n",
      "epoch 86 loss 0.21770616368216983\n",
      "epoch 87 loss 0.2176973259044906\n",
      "epoch 88 loss 0.21768534726510613\n",
      "epoch 89 loss 0.21769310521372295\n",
      "epoch 90 loss 0.2176801734051462\n",
      "epoch 91 loss 0.21767854526386424\n",
      "epoch 92 loss 0.21767222856060933\n",
      "epoch 93 loss 0.21765565240787246\n",
      "epoch 94 loss 0.21764958346799268\n",
      "epoch 95 loss 0.21767258745128826\n",
      "epoch 96 loss 0.21765300454729694\n",
      "epoch 97 loss 0.21762697474431184\n",
      "epoch 98 loss 0.2175946066440162\n",
      "epoch 99 loss 0.21760835915298785\n"
     ]
    }
   ],
   "source": [
    "dim_mnist = 784\n",
    "dims = [784, 300, 300]\n",
    "encoder = get_MLP(dims)\n",
    "decoder = get_MLP(list(reversed(dims)))\n",
    "ae_model = nn.Sequential(encoder, decoder)\n",
    "ae_model = ae_model.to(device)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(ae_model.parameters())\n",
    "ae = Autoencoder(model=ae_model, \n",
    "                loss=loss, optimizer=optimizer)\n",
    "ae.run_train(100, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0025,  0.0034, -0.0259,  ..., -0.0353, -0.0060, -0.0130],\n",
      "        [ 0.0096, -0.0023,  0.0006,  ...,  0.0057,  0.0107, -0.0239],\n",
      "        [ 0.0069,  0.0345,  0.0016,  ...,  0.0049, -0.0037,  0.0266],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0077, -0.0157,  ..., -0.0220,  0.0125,  0.0081],\n",
      "        [-0.0191,  0.0032, -0.0260,  ...,  0.0027,  0.0303, -0.0320],\n",
      "        [-0.0301,  0.0239,  0.0299,  ..., -0.0351,  0.0192,  0.0254]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.1499e-02,  3.5574e-02, -1.5282e-02, -3.4488e-02, -6.6427e-03,\n",
      "        -3.0787e-02,  8.8574e-03, -2.5748e-02,  3.1820e-03, -1.5123e-02,\n",
      "        -3.5610e-02,  3.1792e-03,  1.4448e-02,  1.1403e-02,  3.5713e-02,\n",
      "        -2.4667e-02,  1.9531e-02,  1.6594e-02,  2.1051e-02, -3.0093e-02,\n",
      "         1.0338e-02,  2.0535e-02,  2.8502e-02,  2.2742e-02, -2.7530e-02,\n",
      "         7.9542e-03,  2.6036e-02,  2.7786e-02, -3.9303e-03, -1.9438e-02,\n",
      "         2.2545e-02, -3.3450e-03,  2.9603e-04,  1.2918e-02,  1.8587e-02,\n",
      "         4.1481e-03, -2.1675e-02, -1.3347e-02, -1.1230e-02,  5.3226e-03,\n",
      "         3.5163e-02,  9.4460e-03, -3.1183e-04, -9.0778e-03,  8.4114e-03,\n",
      "        -9.2087e-03,  1.9734e-02,  1.7701e-02,  5.1717e-03, -2.5923e-02,\n",
      "         2.5214e-02,  2.2804e-02, -2.3294e-02,  3.4065e-02, -2.3856e-02,\n",
      "         3.5193e-02,  1.6710e-02,  2.0027e-02,  2.7520e-02, -9.5934e-03,\n",
      "         6.4468e-04,  1.1531e-02, -9.7589e-03, -1.5813e-02, -3.5507e-02,\n",
      "        -1.6383e-02,  1.5854e-02,  1.3377e-02,  1.3953e-02,  1.2710e-02,\n",
      "        -1.3123e-02,  3.4625e-02,  1.0088e-02,  1.4864e-02,  1.6641e-02,\n",
      "         2.9421e-02,  1.5002e-03, -1.5804e-02, -1.7473e-02,  8.8464e-03,\n",
      "         3.2141e-03, -2.3314e-02, -5.5858e-04,  2.8969e-02, -2.9745e-02,\n",
      "        -3.3500e-02, -1.2289e-02,  6.5612e-03,  1.0714e-02,  2.0671e-02,\n",
      "         1.2488e-02,  8.5820e-03, -8.3423e-03,  3.0328e-02, -8.4874e-03,\n",
      "        -2.9532e-02,  2.5774e-02,  1.8763e-02,  2.6720e-02, -1.2254e-02,\n",
      "        -1.4611e-02,  1.6376e-02,  1.6077e-02,  3.0578e-03, -1.5438e-02,\n",
      "         1.5091e-02, -4.5808e-03,  2.0479e-02, -3.8383e-03, -1.5063e-02,\n",
      "         7.1573e-03, -2.6915e-03,  7.7972e-03, -3.3488e-02, -3.4762e-03,\n",
      "        -1.3105e-02, -1.5955e-03, -2.1842e-02, -2.5433e-02,  5.1905e-03,\n",
      "        -1.0398e-04,  1.2501e-02,  1.0848e-02, -1.0520e-05, -2.3194e-02,\n",
      "        -1.8737e-02, -2.7762e-03,  1.2118e-02,  2.3809e-02,  4.8988e-03,\n",
      "        -5.4020e-04,  2.3881e-02, -3.5595e-02,  2.4818e-02,  1.8581e-02,\n",
      "         1.7091e-02, -1.6594e-02,  3.7969e-03,  2.7209e-03, -2.4181e-02,\n",
      "        -1.8527e-02, -2.7713e-02, -1.3972e-02, -3.4096e-02, -1.7413e-02,\n",
      "        -1.9052e-03, -2.4299e-02, -2.7534e-03,  2.0160e-02,  2.1224e-02,\n",
      "         1.4490e-02, -1.1515e-02, -2.6521e-02,  8.2170e-03,  2.5321e-02,\n",
      "         2.1332e-02,  8.4780e-03, -2.7255e-02,  1.4973e-02,  3.3600e-02,\n",
      "        -1.2417e-02,  5.5218e-03, -5.6862e-04,  1.6110e-03, -2.4685e-02,\n",
      "        -1.3139e-02, -2.9298e-02, -3.2230e-02,  2.4786e-02,  6.9130e-03,\n",
      "        -3.1644e-02,  2.4935e-02, -2.9966e-02, -2.6774e-02, -1.2606e-02,\n",
      "        -7.8872e-03, -2.7446e-03, -2.5322e-02, -3.4344e-02,  8.1925e-04,\n",
      "         1.2609e-02,  8.8987e-03,  1.7390e-02,  1.9960e-02,  7.5959e-03,\n",
      "        -1.8808e-02, -3.5456e-02,  1.7937e-02,  2.2236e-03,  2.3611e-02,\n",
      "         6.6158e-03,  2.9387e-02, -2.5664e-02,  3.3508e-02,  8.8963e-03,\n",
      "         9.6455e-03, -5.1228e-03,  2.3520e-02,  3.4793e-02, -2.1691e-02,\n",
      "         3.4899e-02, -1.5307e-02,  5.9330e-03,  1.7019e-02,  5.0882e-03,\n",
      "        -3.3203e-02, -1.6293e-02, -1.5126e-02,  2.7445e-02, -2.0064e-02,\n",
      "        -2.5087e-02,  4.1696e-03, -7.5266e-03,  1.7958e-02,  1.5886e-02,\n",
      "         1.7257e-03,  2.7583e-02,  3.2164e-02,  4.7219e-03,  8.3046e-03,\n",
      "         8.5830e-03,  1.8970e-02, -2.9187e-02,  1.9798e-02, -1.9892e-02,\n",
      "        -2.3960e-02, -2.3328e-03,  8.6830e-03, -1.0097e-02, -3.3010e-02,\n",
      "        -5.2346e-03,  3.1497e-02,  2.7360e-02,  4.3474e-03, -2.4085e-02,\n",
      "         1.1127e-02, -2.3613e-02,  2.3153e-02,  3.3190e-02,  3.3901e-02,\n",
      "         2.8428e-02, -1.5389e-02, -2.3613e-02, -2.2954e-02, -2.3872e-03,\n",
      "         1.1962e-02,  4.4529e-03, -6.3550e-03, -8.0211e-03,  3.5072e-02,\n",
      "         3.1993e-02,  3.3803e-02,  1.0379e-02,  3.3786e-04, -2.9934e-02,\n",
      "        -2.0678e-03, -2.6145e-02, -2.5108e-02,  2.8305e-02, -1.2582e-02,\n",
      "        -2.5109e-03, -3.2238e-02,  2.8858e-02, -2.3769e-02,  2.5342e-02,\n",
      "         1.4844e-02, -2.3102e-03,  1.3654e-02,  2.2087e-04,  2.1634e-02,\n",
      "        -2.6953e-02,  3.4801e-02,  3.3856e-02, -2.6217e-02, -8.0814e-03,\n",
      "        -3.1218e-02,  1.0425e-02,  1.7197e-02, -7.3938e-03, -1.7410e-02,\n",
      "         2.5355e-02, -4.4097e-03, -5.0021e-03, -4.4918e-03, -1.1341e-02,\n",
      "        -1.0984e-02, -7.8092e-03, -2.4056e-02, -2.0340e-02,  1.2536e-02,\n",
      "        -2.7611e-02,  2.1304e-02, -8.1876e-03, -1.6957e-02, -3.1708e-02,\n",
      "        -2.9563e-02,  1.5188e-02, -2.7697e-02,  7.0051e-03, -3.0392e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0311,  0.0277,  0.0025,  ..., -0.0335,  0.0260,  0.0073],\n",
      "        [-0.0421,  0.0422,  0.0258,  ...,  0.0409,  0.0027,  0.0003],\n",
      "        [-0.0204, -0.0084, -0.0062,  ..., -0.0330,  0.0276, -0.0033],\n",
      "        ...,\n",
      "        [-0.0569,  0.0277,  0.0145,  ..., -0.0227, -0.0426, -0.0510],\n",
      "        [ 0.0259, -0.0135, -0.0132,  ...,  0.0402, -0.0473, -0.0254],\n",
      "        [-0.0168,  0.0435,  0.0032,  ..., -0.0326, -0.0034,  0.0206]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 2.5101e-02, -5.0541e-02, -1.0014e-02, -2.1575e-02,  2.4690e-02,\n",
      "        -6.8061e-03,  4.7383e-02, -5.0673e-02, -5.7609e-02,  2.1365e-02,\n",
      "         1.7932e-02,  9.3057e-04, -4.3108e-02, -4.3851e-02,  1.0293e-02,\n",
      "         3.8325e-02,  2.5119e-02, -5.3273e-02,  3.1943e-02, -3.0877e-02,\n",
      "        -1.3993e-02,  4.5027e-02,  4.9419e-02, -2.7526e-02, -1.5701e-02,\n",
      "         5.1976e-02,  3.6150e-02,  4.6345e-02, -4.7476e-02,  2.9564e-02,\n",
      "        -6.0387e-05, -4.9280e-02,  3.0294e-02,  5.6886e-02, -1.6675e-03,\n",
      "        -2.6879e-02, -2.4412e-02,  4.4869e-02, -9.2630e-03,  3.9389e-02,\n",
      "         5.7451e-02,  4.6816e-03,  4.7096e-02,  2.3886e-02,  5.0373e-02,\n",
      "         2.8538e-03, -2.7256e-02,  4.2512e-02,  7.6197e-03, -3.9363e-03,\n",
      "        -4.4845e-02, -1.6471e-02,  1.9393e-03,  4.2362e-02,  1.4833e-02,\n",
      "        -4.6333e-02,  4.6623e-02, -4.1695e-02, -4.7682e-02, -3.8997e-02,\n",
      "         3.6975e-02,  5.2842e-02, -9.9493e-04,  6.3711e-03,  1.7149e-02,\n",
      "        -5.0558e-02, -8.7941e-03, -1.2254e-02,  2.4000e-02, -1.6735e-02,\n",
      "         4.5987e-02,  3.4418e-03, -9.7905e-03, -1.0859e-02,  2.7480e-02,\n",
      "        -5.7411e-02,  3.8544e-02, -2.8637e-02,  5.2129e-02,  3.4843e-02,\n",
      "         5.0478e-02, -2.5654e-02, -1.5344e-02, -2.8939e-02,  2.4290e-02,\n",
      "        -1.6965e-02,  1.7673e-02,  1.0663e-02, -2.7884e-02,  6.8455e-03,\n",
      "         5.4606e-02,  4.0621e-02, -2.2822e-02,  1.8758e-02, -4.0399e-02,\n",
      "        -2.3803e-03,  5.0163e-02,  1.8787e-02, -3.9812e-02,  4.6794e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0874, -0.0793,  0.0564,  ...,  0.0336, -0.0564, -0.0904],\n",
      "        [-0.0953, -0.0394,  0.0437,  ..., -0.0871,  0.0299, -0.0307],\n",
      "        [-0.0351, -0.0480, -0.0075,  ...,  0.0016, -0.0673, -0.0796],\n",
      "        ...,\n",
      "        [ 0.0089, -0.0941, -0.0270,  ..., -0.0975, -0.0306, -0.0476],\n",
      "        [ 0.0663,  0.0384,  0.0430,  ..., -0.0458,  0.0573, -0.0412],\n",
      "        [-0.0692, -0.0400,  0.0036,  ...,  0.0913,  0.0108,  0.0112]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 1.1002e-02, -6.8303e-02,  9.1649e-02, -6.2035e-02, -5.1051e-02,\n",
      "         1.6077e-02, -4.5440e-03,  3.3972e-02,  2.7697e-02, -6.8072e-02,\n",
      "         1.3908e-02,  2.1509e-02,  1.8126e-02, -5.7079e-05,  6.6187e-02,\n",
      "         4.6196e-02, -3.3941e-02,  1.4036e-02,  5.1723e-03, -5.0208e-02,\n",
      "         7.8443e-02,  8.4406e-02,  6.2733e-02,  6.4994e-03,  4.8744e-02,\n",
      "         5.4972e-03, -2.4503e-02, -9.5215e-02, -8.9269e-02,  5.5957e-02,\n",
      "         8.3794e-02, -1.8249e-02,  7.9805e-02, -6.3437e-02, -3.5574e-02,\n",
      "         9.8226e-02,  3.6273e-02, -9.4817e-02, -8.5912e-02,  6.7707e-03,\n",
      "         8.6307e-02, -2.6490e-02,  2.9640e-02, -9.6853e-02,  8.2941e-02,\n",
      "        -3.0191e-02, -5.2839e-02, -1.1593e-02, -7.9604e-02, -1.9282e-02,\n",
      "         3.4559e-02, -4.7377e-02,  7.6565e-02,  8.6037e-02,  8.6984e-02,\n",
      "        -4.1262e-02, -3.6769e-02,  8.9262e-02,  6.7775e-02, -3.2218e-02,\n",
      "         4.5534e-02,  7.9192e-05, -3.1619e-02,  2.0860e-02,  3.6599e-04,\n",
      "        -6.8167e-02, -5.9466e-02, -1.1315e-03, -8.1815e-02,  6.8499e-02,\n",
      "        -3.3790e-03, -8.6040e-02,  3.3434e-02,  1.3236e-03,  9.5155e-02,\n",
      "         8.6099e-02,  8.9701e-02,  5.9339e-03,  1.9893e-02,  9.0217e-02,\n",
      "         9.4255e-02, -7.2120e-02,  8.9451e-02, -8.9974e-02,  4.4729e-02,\n",
      "        -6.8229e-02,  9.1732e-02,  1.8011e-02,  2.5950e-02,  9.8954e-02,\n",
      "         3.0471e-02, -2.7513e-02,  5.2671e-02, -6.1380e-02,  6.4607e-02,\n",
      "         6.2461e-02, -9.6864e-02, -6.9093e-02, -8.8082e-02,  8.7912e-02,\n",
      "        -7.2103e-02,  1.7652e-02, -3.9883e-02,  4.7668e-02, -9.1618e-02,\n",
      "         2.6368e-02,  3.5702e-02,  2.2424e-02,  6.0518e-02,  6.3721e-02,\n",
      "        -5.2132e-02,  6.6663e-02,  3.4394e-02, -7.9525e-02, -9.6808e-02,\n",
      "         5.8373e-02, -6.1935e-02,  7.4833e-02,  1.5543e-02, -5.8395e-02,\n",
      "        -2.8069e-02,  6.9884e-02,  7.7855e-02, -2.0697e-03,  6.7936e-02,\n",
      "         7.9815e-03,  6.4733e-02,  4.1568e-02,  3.3434e-02,  3.1888e-02,\n",
      "        -4.7585e-02,  5.8598e-02, -4.5685e-02,  1.2694e-02,  8.7596e-02,\n",
      "         1.0722e-02, -3.6472e-02,  4.9587e-02,  7.9742e-03, -4.3354e-02,\n",
      "         8.2763e-02, -5.1429e-02,  5.6157e-02,  8.8673e-02,  2.6678e-02,\n",
      "         4.7462e-04,  5.6458e-02,  8.2718e-02,  2.4320e-02, -4.1184e-02,\n",
      "         4.4173e-02,  5.2576e-02, -6.4624e-02, -8.3461e-02,  2.6906e-02,\n",
      "        -5.5157e-02,  5.7185e-02,  9.8577e-02, -2.3539e-02, -6.9058e-02,\n",
      "        -5.3096e-02, -5.4903e-02, -8.0098e-02,  1.3334e-03,  8.2754e-02,\n",
      "         3.8860e-02,  5.0967e-02,  9.9150e-02,  4.8593e-02,  8.1834e-02,\n",
      "         4.2056e-02, -3.7602e-02,  9.4652e-02, -4.6174e-02,  6.2993e-03,\n",
      "         6.3687e-02, -6.5537e-02,  8.8293e-02,  1.6526e-02, -8.2573e-02,\n",
      "        -7.7665e-02,  3.9177e-02,  6.7939e-02, -1.3662e-02,  3.3352e-02,\n",
      "        -5.2017e-02, -3.9525e-02,  6.8695e-02,  1.7081e-02,  1.9405e-02,\n",
      "        -1.5380e-02,  9.6933e-02, -8.6806e-02, -5.7066e-02,  3.4145e-03,\n",
      "        -8.7280e-02, -5.0201e-02,  8.6954e-02, -1.6649e-02,  9.5503e-03,\n",
      "        -3.1847e-03, -2.3102e-02,  9.9232e-02, -7.5056e-02,  6.1914e-02,\n",
      "        -9.1775e-02,  5.9912e-02, -3.2736e-02, -4.4916e-02, -1.7878e-02,\n",
      "         5.1197e-02,  8.6885e-02,  3.2611e-03, -9.7844e-02, -8.0157e-02,\n",
      "        -1.7931e-02,  4.7284e-02,  6.8535e-02,  5.6497e-02, -1.9789e-03,\n",
      "         7.9535e-02,  1.2638e-02,  9.0370e-02,  6.5916e-02,  5.1575e-02,\n",
      "         9.0514e-02,  8.5715e-02, -8.9646e-02, -1.7258e-02, -4.9718e-02,\n",
      "         2.9157e-02,  4.4928e-03, -3.2807e-02,  9.2344e-02, -7.2991e-02,\n",
      "        -6.9457e-02,  3.7401e-02,  1.3384e-02, -7.9111e-02,  6.5128e-03,\n",
      "        -7.9857e-02,  9.0602e-02, -1.0268e-03,  6.5883e-02, -7.2381e-02,\n",
      "         6.6481e-02, -9.4461e-02,  2.9411e-02,  9.4264e-02, -8.0991e-02,\n",
      "        -4.0105e-02, -5.4433e-02, -5.7135e-03,  5.4487e-02, -9.6441e-02,\n",
      "         6.6392e-02, -5.8328e-02, -8.0545e-02,  9.5241e-03,  1.9027e-02,\n",
      "        -1.1010e-02,  8.3560e-02,  9.4664e-02, -4.9418e-02,  5.8386e-03,\n",
      "         5.3272e-02, -2.8102e-02, -7.5568e-02,  5.9607e-02,  1.4139e-02,\n",
      "        -5.0750e-02, -2.8683e-02,  7.9559e-02,  1.5521e-02,  1.2998e-02,\n",
      "         6.9552e-02,  7.8491e-02,  7.0164e-02, -3.6414e-02,  2.7622e-03,\n",
      "         8.7366e-03, -8.2302e-02,  6.1284e-02, -6.3365e-02, -9.4501e-02,\n",
      "        -9.7072e-02,  6.4297e-02, -3.8804e-02,  5.4451e-02,  6.6302e-02,\n",
      "        -3.2243e-02, -2.5930e-02,  4.2145e-02,  9.4622e-03, -6.2413e-02,\n",
      "         3.1395e-02, -4.2852e-02, -6.2170e-02,  3.0647e-02,  9.4541e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0227,  0.0038, -0.0288,  ..., -0.0267,  0.0517,  0.0512],\n",
      "        [ 0.0123, -0.0391,  0.0344,  ..., -0.0303, -0.0204, -0.0315],\n",
      "        [ 0.0349,  0.0356,  0.0448,  ...,  0.0393,  0.0144,  0.0251],\n",
      "        ...,\n",
      "        [ 0.0261,  0.0120, -0.0470,  ...,  0.0523,  0.0422, -0.0277],\n",
      "        [-0.0148, -0.0181,  0.0283,  ...,  0.0478,  0.0022,  0.0157],\n",
      "        [-0.0493,  0.0305,  0.0470,  ...,  0.0286,  0.0136,  0.0365]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0435,  0.0287,  0.0410, -0.0378,  0.0003, -0.0009,  0.0030,  0.0119,\n",
      "         0.0137, -0.0315, -0.0291,  0.0196, -0.0018,  0.0278, -0.0433, -0.0352,\n",
      "         0.0006,  0.0417,  0.0043,  0.0290, -0.0330, -0.0482,  0.0179, -0.0331,\n",
      "        -0.0122,  0.0187, -0.0565, -0.0392,  0.0201,  0.0184, -0.0184,  0.0239,\n",
      "         0.0388, -0.0458,  0.0294,  0.0252,  0.0175, -0.0392,  0.0199,  0.0215,\n",
      "         0.0482, -0.0513,  0.0492, -0.0355, -0.0394, -0.0238, -0.0482, -0.0466,\n",
      "         0.0331, -0.0160,  0.0304, -0.0326, -0.0462,  0.0365, -0.0311,  0.0380,\n",
      "         0.0334,  0.0160,  0.0293,  0.0500, -0.0397,  0.0412, -0.0561, -0.0072,\n",
      "        -0.0509, -0.0307, -0.0292, -0.0082,  0.0410, -0.0112, -0.0075,  0.0048,\n",
      "        -0.0574,  0.0492, -0.0237, -0.0083,  0.0480,  0.0265, -0.0090,  0.0070,\n",
      "         0.0014,  0.0555, -0.0219, -0.0353, -0.0477,  0.0391, -0.0047, -0.0249,\n",
      "         0.0178,  0.0271, -0.0292, -0.0142,  0.0310, -0.0372, -0.0372, -0.0422,\n",
      "        -0.0300,  0.0108, -0.0459, -0.0468, -0.0164,  0.0500, -0.0500,  0.0363,\n",
      "         0.0349, -0.0309,  0.0110,  0.0519,  0.0560, -0.0532,  0.0130,  0.0022,\n",
      "        -0.0291, -0.0183,  0.0068,  0.0048, -0.0273,  0.0569, -0.0089, -0.0437,\n",
      "        -0.0343,  0.0201, -0.0140,  0.0436,  0.0314,  0.0042,  0.0402,  0.0400,\n",
      "         0.0302, -0.0545, -0.0193, -0.0155, -0.0559, -0.0055,  0.0399,  0.0451,\n",
      "         0.0269, -0.0548,  0.0300, -0.0360, -0.0053, -0.0265, -0.0252, -0.0469,\n",
      "        -0.0220,  0.0117,  0.0026,  0.0498,  0.0284, -0.0114, -0.0048,  0.0095,\n",
      "         0.0134,  0.0339, -0.0008, -0.0378,  0.0078,  0.0091,  0.0430,  0.0491,\n",
      "        -0.0070,  0.0167,  0.0421,  0.0126, -0.0173, -0.0082,  0.0307, -0.0554,\n",
      "         0.0154,  0.0577,  0.0503,  0.0096, -0.0233,  0.0246, -0.0465, -0.0083,\n",
      "         0.0360,  0.0084,  0.0436, -0.0517,  0.0272, -0.0434, -0.0553,  0.0514,\n",
      "         0.0140,  0.0026, -0.0201,  0.0548,  0.0057, -0.0397,  0.0539, -0.0142,\n",
      "         0.0567,  0.0062, -0.0036,  0.0571, -0.0356,  0.0253, -0.0478, -0.0520,\n",
      "        -0.0374, -0.0214, -0.0230,  0.0273,  0.0037, -0.0470,  0.0050, -0.0151,\n",
      "        -0.0240,  0.0407,  0.0545, -0.0204, -0.0473, -0.0456,  0.0567,  0.0414,\n",
      "        -0.0168, -0.0244,  0.0019,  0.0392,  0.0265,  0.0434,  0.0171, -0.0009,\n",
      "        -0.0395,  0.0288,  0.0518,  0.0178, -0.0439,  0.0206, -0.0344, -0.0158,\n",
      "        -0.0414, -0.0553,  0.0411, -0.0295, -0.0167,  0.0506,  0.0015,  0.0495,\n",
      "         0.0192,  0.0316, -0.0094,  0.0126,  0.0219,  0.0306, -0.0552,  0.0336,\n",
      "        -0.0430, -0.0098, -0.0020, -0.0423, -0.0308, -0.0406, -0.0251, -0.0149,\n",
      "        -0.0142, -0.0297, -0.0372,  0.0329, -0.0105, -0.0077, -0.0055, -0.0130,\n",
      "        -0.0005,  0.0112,  0.0451,  0.0317,  0.0433, -0.0534, -0.0420, -0.0449,\n",
      "        -0.0086,  0.0434,  0.0256, -0.0287, -0.0230,  0.0123, -0.0221,  0.0212,\n",
      "         0.0160,  0.0287,  0.0104, -0.0353, -0.0567,  0.0179, -0.0041, -0.0453,\n",
      "        -0.0473, -0.0082, -0.0301,  0.0282,  0.0188, -0.0030, -0.0209,  0.0167,\n",
      "        -0.0130, -0.0300,  0.0037, -0.0106, -0.0195,  0.0094, -0.0534, -0.0503,\n",
      "        -0.0535, -0.0286, -0.0177, -0.0311,  0.0437, -0.0434, -0.0115, -0.0441,\n",
      "        -0.0027, -0.0464, -0.0501,  0.0088,  0.0563,  0.0336, -0.0536, -0.0099,\n",
      "         0.0213,  0.0072,  0.0575,  0.0344, -0.0116,  0.0137, -0.0272, -0.0558,\n",
      "         0.0079, -0.0285, -0.0304, -0.0451, -0.0561,  0.0309, -0.0478, -0.0361,\n",
      "         0.0429, -0.0537, -0.0222,  0.0190,  0.0102,  0.0566,  0.0213, -0.0028,\n",
      "        -0.0035, -0.0408, -0.0159,  0.0422, -0.0153, -0.0198,  0.0246,  0.0282,\n",
      "         0.0164,  0.0109,  0.0273,  0.0482,  0.0189, -0.0069, -0.0066,  0.0183,\n",
      "        -0.0053, -0.0204, -0.0316, -0.0338, -0.0239, -0.0474,  0.0500, -0.0010,\n",
      "         0.0558,  0.0364,  0.0424,  0.0238, -0.0272, -0.0537, -0.0479, -0.0184,\n",
      "        -0.0539,  0.0006, -0.0536, -0.0325, -0.0033, -0.0352,  0.0561,  0.0279,\n",
      "        -0.0143, -0.0480, -0.0522, -0.0539,  0.0182, -0.0256, -0.0112,  0.0443,\n",
      "         0.0333, -0.0173,  0.0046, -0.0214, -0.0101,  0.0104, -0.0568,  0.0560,\n",
      "         0.0417, -0.0024,  0.0019, -0.0163, -0.0301, -0.0461,  0.0414, -0.0108,\n",
      "         0.0558, -0.0062, -0.0353, -0.0290, -0.0455, -0.0155,  0.0258,  0.0024,\n",
      "        -0.0310,  0.0152,  0.0441,  0.0065,  0.0124, -0.0403, -0.0337,  0.0425,\n",
      "        -0.0052, -0.0282, -0.0502,  0.0412,  0.0458,  0.0552, -0.0137, -0.0248,\n",
      "        -0.0352,  0.0507,  0.0180, -0.0414,  0.0567,  0.0408, -0.0178,  0.0082,\n",
      "        -0.0534, -0.0345,  0.0463, -0.0073, -0.0001,  0.0377,  0.0082,  0.0413,\n",
      "         0.0400, -0.0380, -0.0234,  0.0267,  0.0405, -0.0202, -0.0524, -0.0364,\n",
      "        -0.0188, -0.0322,  0.0325,  0.0156, -0.0461, -0.0332, -0.0423, -0.0414,\n",
      "        -0.0091,  0.0359, -0.0181, -0.0410, -0.0438,  0.0205, -0.0101, -0.0509,\n",
      "         0.0339, -0.0067,  0.0337, -0.0057,  0.0408, -0.0476, -0.0104, -0.0133,\n",
      "         0.0063,  0.0183,  0.0552, -0.0187,  0.0566,  0.0440, -0.0102,  0.0297,\n",
      "         0.0061, -0.0459, -0.0214, -0.0033, -0.0406,  0.0559, -0.0049, -0.0517,\n",
      "        -0.0362,  0.0277,  0.0099,  0.0084,  0.0440,  0.0460, -0.0293, -0.0426,\n",
      "         0.0435,  0.0482, -0.0296,  0.0147, -0.0548, -0.0131,  0.0342,  0.0206,\n",
      "        -0.0461, -0.0446, -0.0426, -0.0015,  0.0019,  0.0558, -0.0387, -0.0139,\n",
      "         0.0154, -0.0437,  0.0430, -0.0387,  0.0218, -0.0062, -0.0377,  0.0420,\n",
      "         0.0129, -0.0543,  0.0070,  0.0032,  0.0292,  0.0396,  0.0416, -0.0017,\n",
      "        -0.0525, -0.0119, -0.0387, -0.0483,  0.0419,  0.0541, -0.0409, -0.0158,\n",
      "         0.0515, -0.0192, -0.0148, -0.0291, -0.0351,  0.0172, -0.0074, -0.0129,\n",
      "        -0.0362,  0.0382,  0.0494, -0.0420, -0.0540,  0.0268,  0.0532,  0.0055,\n",
      "         0.0394,  0.0043,  0.0330,  0.0128,  0.0050, -0.0043, -0.0478, -0.0130,\n",
      "        -0.0441,  0.0114,  0.0133, -0.0383,  0.0321,  0.0267,  0.0102,  0.0489,\n",
      "         0.0539,  0.0346, -0.0462,  0.0355,  0.0054,  0.0527, -0.0073, -0.0002,\n",
      "         0.0353,  0.0012,  0.0422, -0.0019, -0.0030, -0.0529,  0.0058,  0.0376,\n",
      "        -0.0356,  0.0505, -0.0545, -0.0452,  0.0404,  0.0515, -0.0456,  0.0201,\n",
      "        -0.0470,  0.0458, -0.0537, -0.0388, -0.0231, -0.0293,  0.0486, -0.0399,\n",
      "        -0.0044, -0.0022,  0.0042, -0.0119,  0.0522,  0.0189, -0.0234,  0.0143,\n",
      "         0.0404,  0.0365,  0.0290,  0.0250, -0.0397,  0.0494, -0.0414, -0.0089,\n",
      "         0.0576, -0.0469,  0.0114, -0.0549,  0.0200,  0.0091,  0.0087,  0.0073,\n",
      "        -0.0005,  0.0264,  0.0253,  0.0482, -0.0060, -0.0544, -0.0391, -0.0428,\n",
      "        -0.0200, -0.0311, -0.0461, -0.0049, -0.0180, -0.0546,  0.0472,  0.0346,\n",
      "        -0.0227,  0.0074,  0.0005, -0.0236, -0.0122, -0.0362,  0.0015, -0.0342,\n",
      "         0.0051,  0.0206, -0.0151,  0.0196,  0.0251, -0.0258, -0.0381,  0.0536,\n",
      "         0.0475, -0.0046, -0.0001,  0.0082,  0.0494,  0.0550,  0.0187,  0.0256,\n",
      "         0.0316, -0.0274, -0.0504,  0.0003, -0.0311,  0.0435, -0.0496, -0.0370,\n",
      "        -0.0171, -0.0004, -0.0225,  0.0564,  0.0159, -0.0519,  0.0310, -0.0384,\n",
      "        -0.0035, -0.0458, -0.0407,  0.0348,  0.0218,  0.0533, -0.0153, -0.0197,\n",
      "        -0.0346, -0.0221,  0.0254,  0.0081, -0.0433, -0.0530,  0.0458, -0.0296,\n",
      "         0.0174, -0.0237,  0.0489, -0.0054, -0.0023, -0.0283,  0.0116,  0.0166,\n",
      "        -0.0111, -0.0307, -0.0275,  0.0530, -0.0122,  0.0531, -0.0171, -0.0521,\n",
      "        -0.0114, -0.0295, -0.0284,  0.0175,  0.0314,  0.0395, -0.0261,  0.0073,\n",
      "        -0.0560,  0.0564,  0.0411,  0.0484, -0.0065,  0.0119,  0.0479, -0.0560,\n",
      "         0.0419, -0.0026,  0.0121, -0.0389,  0.0041, -0.0181,  0.0076, -0.0204,\n",
      "         0.0214,  0.0059, -0.0114, -0.0352, -0.0141, -0.0298,  0.0312,  0.0219,\n",
      "        -0.0340, -0.0570,  0.0345, -0.0248,  0.0539, -0.0060, -0.0035, -0.0320,\n",
      "        -0.0412,  0.0020,  0.0329,  0.0411,  0.0145, -0.0392,  0.0102,  0.0243,\n",
      "         0.0064, -0.0463,  0.0295, -0.0528, -0.0418, -0.0341,  0.0410,  0.0119,\n",
      "         0.0127,  0.0354,  0.0378,  0.0400, -0.0408, -0.0316,  0.0155,  0.0159],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x in ae_model.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
