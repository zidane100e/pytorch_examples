{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Transformer(Module):\n",
    "    r\"\"\"A transformer model. User is able to modify the attributes as needed. The architecture\n",
    "    is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer,\n",
    "    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and\n",
    "    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\n",
    "    Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)\n",
    "    model with corresponding parameters.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the encoder/decoder inputs (default=512).\n",
    "        nhead: the number of heads in the multiheadattention models (default=8).\n",
    "        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).\n",
    "        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).\n",
    "        custom_encoder: custom encoder (default=None).\n",
    "        custom_decoder: custom decoder (default=None).\n",
    "\n",
    "    Examples::\n",
    "        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "        >>> src = torch.rand((10, 32, 512))\n",
    "        >>> tgt = torch.rand((20, 32, 512))\n",
    "        >>> out = transformer_model(src, tgt)\n",
    "\n",
    "    Note: A full example to apply nn.Transformer module for the word language model is available in\n",
    "    https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: str = \"relu\", custom_encoder: Optional[Any] = None, \n",
    "                 custom_decoder: Optional[Any] = None) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        if custom_encoder is not None:\n",
    "            self.encoder = custom_encoder\n",
    "        else:\n",
    "            encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            encoder_norm = LayerNorm(d_model)\n",
    "            self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        if custom_decoder is not None:\n",
    "            self.decoder = custom_decoder\n",
    "        else:\n",
    "            decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "            decoder_norm = LayerNorm(d_model)\n",
    "            self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = None, \n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, \n",
    "                src_key_padding_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, \n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Take in and process masked source/target sequences.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder (required).\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            src_mask: the additive mask for the src sequence (optional).\n",
    "            tgt_mask: the additive mask for the tgt sequence (optional).\n",
    "            memory_mask: the additive mask for the encoder output (optional).\n",
    "            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n",
    "            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            - src: :math:`(S, N, E)`.\n",
    "            - tgt: :math:`(T, N, E)`.\n",
    "            - src_mask: :math:`(S, S)`.\n",
    "            - tgt_mask: :math:`(T, T)`.\n",
    "            - memory_mask: :math:`(T, S)`.\n",
    "            - src_key_padding_mask: :math:`(N, S)`.\n",
    "            - tgt_key_padding_mask: :math:`(N, T)`.\n",
    "            - memory_key_padding_mask: :math:`(N, S)`.\n",
    "\n",
    "            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n",
    "            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n",
    "            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n",
    "            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "            is provided, it will be added to the attention weight. \n",
    "            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n",
    "            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n",
    "            positions will be unchanged. If a BoolTensor is provided, the positions with the\n",
    "            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "\n",
    "            - output: :math:`(T, N, E)`.\n",
    "\n",
    "            Note: Due to the multi-head attention architecture in the transformer model,\n",
    "            the output sequence length of a transformer is same as the input sequence\n",
    "            (i.e. target) length of the decode.\n",
    "\n",
    "            where S is the source sequence length, T is the target sequence length, N is the\n",
    "            batch size, E is the feature number\n",
    "\n",
    "        Examples:\n",
    "            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        \"\"\"\n",
    "\n",
    "        if src.size(1) != tgt.size(1):\n",
    "            raise RuntimeError(\"the batch number of src and tgt must be equal\")\n",
    "\n",
    "        if src.size(2) != self.d_model or tgt.size(2) != self.d_model:\n",
    "            raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")\n",
    "\n",
    "        memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                              tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                              memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
